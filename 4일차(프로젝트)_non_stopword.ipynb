{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4일차(프로젝트)_non_stopword.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YfLt2uU0G3XFc27snKrNyZx6I4rRIID3",
      "authorship_tag": "ABX9TyNh7MutJ1QbpPXrcukw8aNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bowwow-carrot/self_study_machine_learning-deep_learning/blob/main/4%EC%9D%BC%EC%B0%A8(%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8)_non_stopword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4TfB3NX-CXB"
      },
      "source": [
        "\"\"\"\n",
        "!pip install konlpy\n",
        " \n",
        "프로젝트 주제 : 한국어 감정 분석기 \n",
        "1 . 한국어 형태소 분석기를 활용한 , 전처리 \n",
        "[[아 더빙 진짜 짜증, 0] , ['흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1\\n']\n",
        "train  = [[아, 더빙, 진짜, 짜증] , [흠 , .. , 포스터, 보, 고 , 초딩, 영화..].....]\n",
        "label = [0,1.....]\n",
        "\n",
        "2 . 각 토큰이 자주 등장한 단어 순위\n",
        "2 - 1 각 단어(토큰) 의 빈도수 체크\n",
        " ex)\n",
        "vindos['아'] = 3000\n",
        "vindos['진짜'] = 4200\n",
        "\n",
        "2 - 2 빈도수 큰것부터 인덱스화 \n",
        "ex)\n",
        "'진짜' = 0\n",
        "'아' = 1\n",
        "train = [[0,32,1,54],[1,8,3,23]]\n",
        "label = [0,1]\n",
        "\n",
        "2 - 3 인덱스와 value 매칭이 되는 vocab dictionary\n",
        "아 정말 짜증난다 -> [0,1,2,3]\n",
        "\n",
        "---- 전처리 종료 ----------\n",
        "3 - 1\n",
        "sklearn - train, test split,  위 전처리한 파일을 분리\n",
        "8 : 2\n",
        "-> 코드제출\n",
        "-> 예시 제출\n",
        "\n",
        "3 - 2\n",
        "학습 진행 , \n",
        "vocab size, sequence 길이 조절\n",
        "model epoch 10\n",
        "validation data accuracy 57% \n",
        "-> 제출 로그파일 \n",
        "\n",
        "\"\"\"\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYMA3K8Q_rvW",
        "outputId": "b2b131d4-8d9c-47cc-f517-5b387b9b0c9f"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFBbjFsJujZu"
      },
      "source": [
        "import os\n",
        "import os.path as pth\n",
        "\n",
        "#설명 추가해야됨.\n",
        "from glob import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from keras.preprocessing.sequence import pad_sequences as p_seq\n",
        "from tensorflow.keras.optimizers import Adam as adam\n",
        "from keras import layers, models\n",
        "\n",
        "from konlpy.tag import  Komoran\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLFGWUg1vfem"
      },
      "source": [
        "MODEL_NAME = 'RNN_test'\n",
        "MODEL_PATH = '/content/drive/MyDrive/ai_project/RNN_project_data/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWi6ehkR-9EC"
      },
      "source": [
        "komoran = Komoran()\n",
        "\n",
        "#/content/drive/MyDrive/ai_project/ratings_train.txt\n",
        "#/content/ratings_train.txt\n",
        "# 에러코드정리\n",
        "#   UnicodeDecodeError: 'utf-8' codec can't decode byte 0xec in position 0: unexpected end of data :: UTF-16 코드\n",
        "#     xFF xFE(0xFFFE)는 BOM(Byte order Mark)라고 한다고 한다.\n",
        "#     16비트 문자의 경우 빅엔디안(0xFE FF)인지, 리틀엔디안(0xFF FE)인지를 구분하기 위한 헤더\n",
        "#   UnicodeError: UTF-16 stream does not start with BOM :: cp949 인코드\n",
        "with open('/content/drive/MyDrive/ai_project/ratings_train_utf8(DOM).txt', mode='r',encoding='UTF-8') as data:\n",
        "  all_line = data.readlines()\n",
        "  train = [] #앞으로의 y 값\n",
        "  label = []\n",
        "  vocab = []\n",
        "  #'id\\tdocument\\tlabel\\n', '9976970\\t아 더빙.. 진짜 짜증나네요 목소리\\t0\\n',\n",
        "  count = 0\n",
        "  for i in all_line :\n",
        "    if count == 20000: #layers.Embedding(max_features, 128)(x) 에 크기를 맞춰줘야됨.\n",
        "      break\n",
        "    i = i.split('\\t')\n",
        "    # [id, document,label]\n",
        "    vocab.append(i[1])\n",
        "    token=komoran.morphs(i[1]) #morphs 로 내용 학습\n",
        "    train.append(token)\n",
        "    label_num=i[2].rstrip('\\n') \n",
        "    label.append(label_num)\n",
        "    count = count + 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uOwOfof-5wp"
      },
      "source": [
        "# 주 목표 : 위의 x_train과 y_train처럼 만들어주는 것이 목표.\n",
        "#2 - 1 각 단어(토큰) 의 빈도수 체크\n",
        "vindos = {}\n",
        "for i in train:\n",
        "  for j in i :\n",
        "    if j in vindos:\n",
        "      vindos[j] = vindos[j]+1\n",
        "    else :\n",
        "      vindos[j] = 1\n",
        "\n",
        "vindos_list1 = [[k,v] for k, v in zip(vindos.keys(),vindos.values())]\n",
        "vindos_key = list(vindos)\n",
        "vindos_val = list(vindos.values())\n",
        "\n",
        "\n",
        "#2 - 2 빈도수 큰것부터 인덱스화 \n",
        "test_train = [[vindos[j] for j in i] for i in train]\n",
        "for i in test_train:\n",
        "  i.sort(reverse=True)\n",
        "\n",
        "#2 - 3 인덱스와 value 매칭이 되는 vocab dictionary\n",
        "#아 정말 짜증난다 -> [0,1,2,3]\n",
        "\n",
        "vocab_dict = {k:v for k,v in zip(vocab, test_train)}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySYIGRxZLvGo"
      },
      "source": [
        "#2 - 3 인덱스와 value 매칭이 되는 vocab dictionary\n",
        "\n",
        "#test_train\n",
        "\n",
        "#test_dict = {'아 정말 짜증난다' : [0,1,2,3]}\n",
        "#test_dict = {'test':test_train[2]}\n",
        "#test_dict\n",
        "\n",
        "#vocab_dict = {k:v for k,v in zip(vocab, test_train)}\n",
        "#vocab_dict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDm7B_lNQMKE",
        "outputId": "ad5fc34c-fc77-41a3-be4e-a8c7711de195"
      },
      "source": [
        "#3 - 1 sklearn - train, test split,  위 전처리한 파일을 분리 8 : 2 -> 코드제출 -> 예시 제출\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from keras.preprocessing.sequence import pad_sequences as p_seq\n",
        "from tensorflow.keras.optimizers import Adam as adam\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "\"\"\"\n",
        "#TPU 사용\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "# 훈련을 여러 GPU 또는 여러 장비, 여러 TPU로 나누어 처리하기 위한 텐서플로 API\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "max_features=20000\n",
        "maxlen=100\n",
        "\n",
        "test_label = np.array(label, dtype='int')\n",
        "x_train, x_test, y_train, y_test = tts(test_train, test_label, test_size=0.2, random_state=42)\n",
        "print(f\"x_train:{len(x_train)}, y_train:{len(y_train)}, x_test:{len(x_test)}, y_test:{len(y_test)}\")\n",
        "x_train = p_seq(x_train, maxlen=maxlen)\n",
        "x_test = p_seq(x_test, maxlen=maxlen)\n",
        "#y_train = np.array(y_train, dtype='int')\n",
        "#y_test = np.array(y_test, dtype='int')\n",
        "print(f\"x_train:{len(x_train)}, y_train:{len(y_train)}, x_test:{len(x_test)}, y_test:{len(y_test)}\")\n",
        "\n",
        "#3 - 2 학습 진행 , vocab size, sequence 길이 조절 model epoch 10 validation data accuracy 57%  -> 제출 로그파일 \n",
        "#x = layers.Input((maxlen,)) #maxlen만큼 입력노드 생성\n",
        "#h = layers.Embedding(max_features, 128)(x)\n",
        "#h = layers.SimpleRNN(128)(h)\n",
        "#y = layers.Dense(1, activation='sigmoid')(h)\n",
        "\n",
        "#model = models.Model(x,y)\n",
        "\n",
        "\n",
        "\n",
        "#keras.optimizers.Adam()\n",
        "# Learning rate 0.1\n",
        "# beta 1, 2 => beta 1 Momentum 0.9 ,  beta 2 Adagrad 0.99\n",
        "# epsilon => Adagrad 분모 가 0 되는 것을 방지하기 위한 파라메터 0.00001\n",
        "# decay => Learning rate 1 epoch 진행 마다 Decay 비율만큼 감소 0.01 or 0.0\n",
        "# Learning rate = 0.1 , decay 0.01 \n",
        "# 1 Epoch LR  = 0.1\n",
        "# 2 Epoch LR  = 0.09\n",
        "\n",
        "#adam의 옵션!!\n",
        "#keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999,epsilon=none,amsgrad=False)\n",
        "# epochs = 4 => \n",
        "\"\"\"\n",
        "learning_rate: 0보다 크거나 같은 float 값. 학습률.\n",
        "beta_1: 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정됩니다.\n",
        "beta_2: 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정됩니다.\n",
        "epsilon: 0보다 크거나 같은 float형 fuzz factor. None인 경우 K.epsilon()이 사용됩니다.\n",
        "decay: 0보다 크거나 같은 float 값. 업데이트마다 적용되는 학습률의 감소율입니다.\n",
        "amsgrad: 불리언. Adam의 변형인 AMSGrad의 적용 여부를 설정합니다. AMSGrad는 \"On the Convergence of Adam and Beyond\" 논문에서 소개되었습니다.\n",
        "\"\"\"\n",
        "\n",
        "with strategy.scope():\n",
        "  x = layers.Input((maxlen,)) #maxlen만큼 입력노드 생성\n",
        "  h = layers.Embedding(max_features, 128)(x)\n",
        "  h = layers.SimpleRNN(128)(h)\n",
        "  y = layers.Dense(1, activation='sigmoid')(h)\n",
        "  model = models.Model(x,y)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.001),metrics=['accuracy'])\n",
        "\n",
        "#val_loss가 적은 값이 적은 모델 저장\n",
        "checkpoint_path = pth.join(MODEL_PATH, MODEL_NAME)\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "model_file_path = pth.join(checkpoint_path, 'Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5')\n",
        "checkpoint = ModelCheckpoint(filepath=model_file_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# {patience}회간 val_loss가 좋아지지 않으면 중지.\n",
        "#early_stopping = EarlyStopping(monitor='val_loss',patience=30)\n",
        "\n",
        "batch_size = 1000\n",
        "epochs = 300\n",
        "history = model.fit(x_train, y_train,\n",
        "           batch_size = batch_size,\n",
        "           epochs = epochs,\n",
        "           validation_data=(x_test, y_test),\n",
        "           #callbacks=[checkpoint, early_stopping],\n",
        "           callbacks=[checkpoint],\n",
        "           )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.38.166.154:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.38.166.154:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.38.166.154:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.38.166.154:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train:16000, y_train:16000, x_test:4000, y_test:4000\n",
            "x_train:16000, y_train:16000, x_test:4000, y_test:4000\n",
            "Epoch 1/300\n",
            "16/16 [==============================] - 5s 107ms/step - loss: 0.7302 - accuracy: 0.4978 - val_loss: 0.6975 - val_accuracy: 0.4930\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69753, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_001_Val_0.698.hdf5\n",
            "Epoch 2/300\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 0.6900 - accuracy: 0.5218 - val_loss: 0.6831 - val_accuracy: 0.5483\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69753 to 0.68315, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_002_Val_0.683.hdf5\n",
            "Epoch 3/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6485 - accuracy: 0.6072 - val_loss: 0.6906 - val_accuracy: 0.6455\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68315\n",
            "Epoch 4/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6909 - accuracy: 0.5509 - val_loss: 0.6723 - val_accuracy: 0.5670\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.68315 to 0.67235, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_004_Val_0.672.hdf5\n",
            "Epoch 5/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6274 - accuracy: 0.6390 - val_loss: 0.6478 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.67235 to 0.64780, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_005_Val_0.648.hdf5\n",
            "Epoch 6/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5805 - accuracy: 0.7048 - val_loss: 0.5959 - val_accuracy: 0.6883\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.64780 to 0.59591, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_006_Val_0.596.hdf5\n",
            "Epoch 7/300\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 0.5513 - accuracy: 0.7331 - val_loss: 0.5709 - val_accuracy: 0.6995\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.59591 to 0.57092, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_007_Val_0.571.hdf5\n",
            "Epoch 8/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5314 - accuracy: 0.7376 - val_loss: 0.5252 - val_accuracy: 0.7395\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.57092 to 0.52520, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_008_Val_0.525.hdf5\n",
            "Epoch 9/300\n",
            "16/16 [==============================] - 1s 70ms/step - loss: 0.5082 - accuracy: 0.7519 - val_loss: 0.5274 - val_accuracy: 0.7523\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.52520\n",
            "Epoch 10/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4950 - accuracy: 0.7638 - val_loss: 0.5138 - val_accuracy: 0.7490\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.52520 to 0.51385, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_010_Val_0.514.hdf5\n",
            "Epoch 11/300\n",
            "16/16 [==============================] - 2s 134ms/step - loss: 0.4946 - accuracy: 0.7508 - val_loss: 0.5232 - val_accuracy: 0.7328\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.51385\n",
            "Epoch 12/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4839 - accuracy: 0.7697 - val_loss: 0.5081 - val_accuracy: 0.7545\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.51385 to 0.50815, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_012_Val_0.508.hdf5\n",
            "Epoch 13/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.5120 - accuracy: 0.7440 - val_loss: 0.5353 - val_accuracy: 0.7400\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.50815\n",
            "Epoch 14/300\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.4866 - accuracy: 0.7679 - val_loss: 0.5173 - val_accuracy: 0.7528\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.50815\n",
            "Epoch 15/300\n",
            "16/16 [==============================] - 1s 91ms/step - loss: 0.4667 - accuracy: 0.7778 - val_loss: 0.5168 - val_accuracy: 0.7410\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.50815\n",
            "Epoch 16/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4728 - accuracy: 0.7736 - val_loss: 0.5119 - val_accuracy: 0.7488\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.50815\n",
            "Epoch 17/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4594 - accuracy: 0.7825 - val_loss: 0.5079 - val_accuracy: 0.7495\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.50815 to 0.50788, saving model to /content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_017_Val_0.508.hdf5\n",
            "Epoch 18/300\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 0.4601 - accuracy: 0.7860 - val_loss: 0.5318 - val_accuracy: 0.7373\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.50788\n",
            "Epoch 19/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4621 - accuracy: 0.7711 - val_loss: 0.5468 - val_accuracy: 0.7183\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.50788\n",
            "Epoch 20/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4883 - accuracy: 0.7628 - val_loss: 0.5449 - val_accuracy: 0.7485\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.50788\n",
            "Epoch 21/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4587 - accuracy: 0.7759 - val_loss: 0.5714 - val_accuracy: 0.7303\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.50788\n",
            "Epoch 22/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.4771 - accuracy: 0.7716 - val_loss: 0.5744 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.50788\n",
            "Epoch 23/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4925 - accuracy: 0.7494 - val_loss: 0.5401 - val_accuracy: 0.7340\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.50788\n",
            "Epoch 24/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4484 - accuracy: 0.7907 - val_loss: 0.5567 - val_accuracy: 0.7215\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.50788\n",
            "Epoch 25/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.4653 - accuracy: 0.7672 - val_loss: 0.5395 - val_accuracy: 0.7068\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.50788\n",
            "Epoch 26/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4524 - accuracy: 0.7792 - val_loss: 0.5430 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.50788\n",
            "Epoch 27/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4410 - accuracy: 0.7906 - val_loss: 0.5616 - val_accuracy: 0.7350\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.50788\n",
            "Epoch 28/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4331 - accuracy: 0.7946 - val_loss: 0.5468 - val_accuracy: 0.7420\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.50788\n",
            "Epoch 29/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4205 - accuracy: 0.8022 - val_loss: 0.5444 - val_accuracy: 0.7433\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.50788\n",
            "Epoch 30/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4136 - accuracy: 0.8031 - val_loss: 0.5641 - val_accuracy: 0.7435\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.50788\n",
            "Epoch 31/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.4260 - accuracy: 0.8033 - val_loss: 0.5526 - val_accuracy: 0.7453\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.50788\n",
            "Epoch 32/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4175 - accuracy: 0.8056 - val_loss: 0.5622 - val_accuracy: 0.7348\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.50788\n",
            "Epoch 33/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4055 - accuracy: 0.8115 - val_loss: 0.5836 - val_accuracy: 0.7370\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.50788\n",
            "Epoch 34/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4044 - accuracy: 0.8114 - val_loss: 0.5645 - val_accuracy: 0.7378\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.50788\n",
            "Epoch 35/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3966 - accuracy: 0.8164 - val_loss: 0.5677 - val_accuracy: 0.7270\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.50788\n",
            "Epoch 36/300\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.3991 - accuracy: 0.8178 - val_loss: 0.6116 - val_accuracy: 0.7368\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.50788\n",
            "Epoch 37/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4209 - accuracy: 0.8001 - val_loss: 0.5859 - val_accuracy: 0.7408\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.50788\n",
            "Epoch 38/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4064 - accuracy: 0.8121 - val_loss: 0.5589 - val_accuracy: 0.7315\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.50788\n",
            "Epoch 39/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4269 - accuracy: 0.7998 - val_loss: 0.5924 - val_accuracy: 0.7265\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.50788\n",
            "Epoch 40/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4281 - accuracy: 0.8008 - val_loss: 0.5669 - val_accuracy: 0.7265\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.50788\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4053 - accuracy: 0.8126 - val_loss: 0.5711 - val_accuracy: 0.7353\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.50788\n",
            "Epoch 42/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3942 - accuracy: 0.8199 - val_loss: 0.6084 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.50788\n",
            "Epoch 43/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.4236 - accuracy: 0.8074 - val_loss: 0.6521 - val_accuracy: 0.6928\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.50788\n",
            "Epoch 44/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.5176 - accuracy: 0.7204 - val_loss: 0.5635 - val_accuracy: 0.7170\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.50788\n",
            "Epoch 45/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4526 - accuracy: 0.7808 - val_loss: 0.5533 - val_accuracy: 0.7300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.50788\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4379 - accuracy: 0.7935 - val_loss: 0.5666 - val_accuracy: 0.7300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.50788\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4114 - accuracy: 0.8113 - val_loss: 0.5703 - val_accuracy: 0.7318\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.50788\n",
            "Epoch 48/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4608 - accuracy: 0.7719 - val_loss: 0.6033 - val_accuracy: 0.7183\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.50788\n",
            "Epoch 49/300\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 0.4412 - accuracy: 0.7889 - val_loss: 0.5651 - val_accuracy: 0.7360\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50788\n",
            "Epoch 50/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4202 - accuracy: 0.8011 - val_loss: 0.5674 - val_accuracy: 0.7378\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50788\n",
            "Epoch 51/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4004 - accuracy: 0.8150 - val_loss: 0.5590 - val_accuracy: 0.7230\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.50788\n",
            "Epoch 52/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3914 - accuracy: 0.8216 - val_loss: 0.5767 - val_accuracy: 0.7235\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50788\n",
            "Epoch 53/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3772 - accuracy: 0.8283 - val_loss: 0.5783 - val_accuracy: 0.7293\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.50788\n",
            "Epoch 54/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3703 - accuracy: 0.8309 - val_loss: 0.5891 - val_accuracy: 0.7345\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.50788\n",
            "Epoch 55/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3680 - accuracy: 0.8303 - val_loss: 0.6056 - val_accuracy: 0.7163\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.50788\n",
            "Epoch 56/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.3577 - accuracy: 0.8374 - val_loss: 0.6046 - val_accuracy: 0.7260\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.50788\n",
            "Epoch 57/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3521 - accuracy: 0.8444 - val_loss: 0.6146 - val_accuracy: 0.7173\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.50788\n",
            "Epoch 58/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3584 - accuracy: 0.8404 - val_loss: 0.6194 - val_accuracy: 0.7223\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.50788\n",
            "Epoch 59/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.3997 - accuracy: 0.8129 - val_loss: 0.6112 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.50788\n",
            "Epoch 60/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4689 - accuracy: 0.7769 - val_loss: 0.6113 - val_accuracy: 0.6880\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.50788\n",
            "Epoch 61/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4284 - accuracy: 0.7966 - val_loss: 0.5851 - val_accuracy: 0.7180\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.50788\n",
            "Epoch 62/300\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.3937 - accuracy: 0.8178 - val_loss: 0.5719 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.50788\n",
            "Epoch 63/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3784 - accuracy: 0.8264 - val_loss: 0.5949 - val_accuracy: 0.7200\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.50788\n",
            "Epoch 64/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3686 - accuracy: 0.8327 - val_loss: 0.6202 - val_accuracy: 0.7268\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.50788\n",
            "Epoch 65/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3664 - accuracy: 0.8319 - val_loss: 0.6029 - val_accuracy: 0.7250\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.50788\n",
            "Epoch 66/300\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 0.3929 - accuracy: 0.8159 - val_loss: 0.6436 - val_accuracy: 0.6995\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.50788\n",
            "Epoch 67/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4050 - accuracy: 0.8117 - val_loss: 0.6407 - val_accuracy: 0.7085\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.50788\n",
            "Epoch 68/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3764 - accuracy: 0.8263 - val_loss: 0.6172 - val_accuracy: 0.7128\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.50788\n",
            "Epoch 69/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3835 - accuracy: 0.8200 - val_loss: 0.6168 - val_accuracy: 0.7190\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.50788\n",
            "Epoch 70/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3585 - accuracy: 0.8367 - val_loss: 0.6265 - val_accuracy: 0.7250\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.50788\n",
            "Epoch 71/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3426 - accuracy: 0.8468 - val_loss: 0.6425 - val_accuracy: 0.7338\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.50788\n",
            "Epoch 72/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3578 - accuracy: 0.8368 - val_loss: 0.6576 - val_accuracy: 0.7253\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.50788\n",
            "Epoch 73/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.7052 - accuracy: 0.6579 - val_loss: 0.6203 - val_accuracy: 0.6533\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.50788\n",
            "Epoch 74/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.7197 - accuracy: 0.6323 - val_loss: 0.6392 - val_accuracy: 0.6738\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.50788\n",
            "Epoch 75/300\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.5964 - accuracy: 0.7010 - val_loss: 0.6191 - val_accuracy: 0.6693\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.50788\n",
            "Epoch 76/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5442 - accuracy: 0.7245 - val_loss: 0.5882 - val_accuracy: 0.7088\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.50788\n",
            "Epoch 77/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5091 - accuracy: 0.7544 - val_loss: 0.5715 - val_accuracy: 0.7175\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.50788\n",
            "Epoch 78/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4934 - accuracy: 0.7636 - val_loss: 0.5701 - val_accuracy: 0.7298\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.50788\n",
            "Epoch 79/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4862 - accuracy: 0.7652 - val_loss: 0.5666 - val_accuracy: 0.7185\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.50788\n",
            "Epoch 80/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4788 - accuracy: 0.7674 - val_loss: 0.5697 - val_accuracy: 0.7148\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.50788\n",
            "Epoch 81/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4619 - accuracy: 0.7808 - val_loss: 0.5598 - val_accuracy: 0.7325\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.50788\n",
            "Epoch 82/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4525 - accuracy: 0.7908 - val_loss: 0.5634 - val_accuracy: 0.7205\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.50788\n",
            "Epoch 83/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4532 - accuracy: 0.7850 - val_loss: 0.5681 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.50788\n",
            "Epoch 84/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.4692 - accuracy: 0.7751 - val_loss: 0.5679 - val_accuracy: 0.7238\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.50788\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4590 - accuracy: 0.7824 - val_loss: 0.5695 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.50788\n",
            "Epoch 86/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4517 - accuracy: 0.7884 - val_loss: 0.5676 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.50788\n",
            "Epoch 87/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4416 - accuracy: 0.7948 - val_loss: 0.5594 - val_accuracy: 0.7345\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.50788\n",
            "Epoch 88/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4255 - accuracy: 0.8036 - val_loss: 0.5598 - val_accuracy: 0.7278\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.50788\n",
            "Epoch 89/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4168 - accuracy: 0.8095 - val_loss: 0.5627 - val_accuracy: 0.7273\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.50788\n",
            "Epoch 90/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4099 - accuracy: 0.8114 - val_loss: 0.5684 - val_accuracy: 0.7175\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.50788\n",
            "Epoch 91/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4126 - accuracy: 0.8058 - val_loss: 0.5796 - val_accuracy: 0.7258\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.50788\n",
            "Epoch 92/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4009 - accuracy: 0.8178 - val_loss: 0.5854 - val_accuracy: 0.7295\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50788\n",
            "Epoch 93/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.4047 - accuracy: 0.8167 - val_loss: 0.6078 - val_accuracy: 0.7145\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50788\n",
            "Epoch 94/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3980 - accuracy: 0.8197 - val_loss: 0.5893 - val_accuracy: 0.7295\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50788\n",
            "Epoch 95/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3831 - accuracy: 0.8264 - val_loss: 0.5927 - val_accuracy: 0.7288\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50788\n",
            "Epoch 96/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3733 - accuracy: 0.8321 - val_loss: 0.5988 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50788\n",
            "Epoch 97/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.3684 - accuracy: 0.8373 - val_loss: 0.5921 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50788\n",
            "Epoch 98/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3625 - accuracy: 0.8404 - val_loss: 0.5967 - val_accuracy: 0.7148\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50788\n",
            "Epoch 99/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3552 - accuracy: 0.8444 - val_loss: 0.6116 - val_accuracy: 0.7238\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50788\n",
            "Epoch 100/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3520 - accuracy: 0.8447 - val_loss: 0.6098 - val_accuracy: 0.7235\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.50788\n",
            "Epoch 101/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.3419 - accuracy: 0.8517 - val_loss: 0.6223 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.50788\n",
            "Epoch 102/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3857 - accuracy: 0.8354 - val_loss: 0.6787 - val_accuracy: 0.7035\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.50788\n",
            "Epoch 103/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5329 - accuracy: 0.7549 - val_loss: 0.6394 - val_accuracy: 0.7273\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.50788\n",
            "Epoch 104/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5471 - accuracy: 0.7384 - val_loss: 0.7383 - val_accuracy: 0.6203\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.50788\n",
            "Epoch 105/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.6227 - accuracy: 0.6891 - val_loss: 0.6756 - val_accuracy: 0.6513\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.50788\n",
            "Epoch 106/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.5873 - accuracy: 0.7071 - val_loss: 0.6195 - val_accuracy: 0.6928\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.50788\n",
            "Epoch 107/300\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.5530 - accuracy: 0.7359 - val_loss: 0.5977 - val_accuracy: 0.7155\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.50788\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5851 - accuracy: 0.7083 - val_loss: 0.6457 - val_accuracy: 0.6623\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.50788\n",
            "Epoch 109/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5625 - accuracy: 0.7214 - val_loss: 0.5908 - val_accuracy: 0.7153\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.50788\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5314 - accuracy: 0.7468 - val_loss: 0.5882 - val_accuracy: 0.7138\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.50788\n",
            "Epoch 111/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5198 - accuracy: 0.7486 - val_loss: 0.5829 - val_accuracy: 0.7153\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.50788\n",
            "Epoch 112/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5096 - accuracy: 0.7558 - val_loss: 0.5671 - val_accuracy: 0.7303\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.50788\n",
            "Epoch 113/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.5251 - accuracy: 0.7511 - val_loss: 0.5793 - val_accuracy: 0.7273\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.50788\n",
            "Epoch 114/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5026 - accuracy: 0.7591 - val_loss: 0.5778 - val_accuracy: 0.7263\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.50788\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5536 - accuracy: 0.7246 - val_loss: 0.7585 - val_accuracy: 0.5843\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.50788\n",
            "Epoch 116/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6956 - accuracy: 0.6016 - val_loss: 0.6721 - val_accuracy: 0.6005\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.50788\n",
            "Epoch 117/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.6521 - accuracy: 0.6064 - val_loss: 0.6641 - val_accuracy: 0.5990\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.50788\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.6316 - accuracy: 0.6274 - val_loss: 0.6559 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.50788\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.6168 - accuracy: 0.6404 - val_loss: 0.6435 - val_accuracy: 0.6260\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.50788\n",
            "Epoch 120/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.6023 - accuracy: 0.6574 - val_loss: 0.6427 - val_accuracy: 0.6253\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50788\n",
            "Epoch 121/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5925 - accuracy: 0.6668 - val_loss: 0.6364 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50788\n",
            "Epoch 122/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5817 - accuracy: 0.6793 - val_loss: 0.6388 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50788\n",
            "Epoch 123/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5740 - accuracy: 0.6861 - val_loss: 0.6358 - val_accuracy: 0.6453\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50788\n",
            "Epoch 124/300\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.5697 - accuracy: 0.6883 - val_loss: 0.6324 - val_accuracy: 0.6500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.50788\n",
            "Epoch 125/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5655 - accuracy: 0.6918 - val_loss: 0.6342 - val_accuracy: 0.6490\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.50788\n",
            "Epoch 126/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5614 - accuracy: 0.6960 - val_loss: 0.6367 - val_accuracy: 0.6460\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.50788\n",
            "Epoch 127/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.5559 - accuracy: 0.7024 - val_loss: 0.6285 - val_accuracy: 0.6540\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.50788\n",
            "Epoch 128/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5489 - accuracy: 0.7089 - val_loss: 0.6267 - val_accuracy: 0.6608\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.50788\n",
            "Epoch 129/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5441 - accuracy: 0.7148 - val_loss: 0.6321 - val_accuracy: 0.6575\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.50788\n",
            "Epoch 130/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.5419 - accuracy: 0.7110 - val_loss: 0.6319 - val_accuracy: 0.6588\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.50788\n",
            "Epoch 131/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5384 - accuracy: 0.7154 - val_loss: 0.6282 - val_accuracy: 0.6613\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.50788\n",
            "Epoch 132/300\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 0.5321 - accuracy: 0.7238 - val_loss: 0.6291 - val_accuracy: 0.6705\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.50788\n",
            "Epoch 133/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5274 - accuracy: 0.7253 - val_loss: 0.6240 - val_accuracy: 0.6708\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.50788\n",
            "Epoch 134/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5243 - accuracy: 0.7311 - val_loss: 0.6218 - val_accuracy: 0.6710\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.50788\n",
            "Epoch 135/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5201 - accuracy: 0.7319 - val_loss: 0.6254 - val_accuracy: 0.6760\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.50788\n",
            "Epoch 136/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5134 - accuracy: 0.7411 - val_loss: 0.6275 - val_accuracy: 0.6725\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.50788\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5181 - accuracy: 0.7348 - val_loss: 0.6369 - val_accuracy: 0.6730\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.50788\n",
            "Epoch 138/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5485 - accuracy: 0.7136 - val_loss: 0.6394 - val_accuracy: 0.6620\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.50788\n",
            "Epoch 139/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5447 - accuracy: 0.7147 - val_loss: 0.6364 - val_accuracy: 0.6643\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.50788\n",
            "Epoch 140/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5356 - accuracy: 0.7200 - val_loss: 0.6365 - val_accuracy: 0.6650\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.50788\n",
            "Epoch 141/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.5340 - accuracy: 0.7240 - val_loss: 0.6243 - val_accuracy: 0.6668\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.50788\n",
            "Epoch 142/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5287 - accuracy: 0.7293 - val_loss: 0.6195 - val_accuracy: 0.6688\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.50788\n",
            "Epoch 143/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5243 - accuracy: 0.7339 - val_loss: 0.6151 - val_accuracy: 0.6773\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.50788\n",
            "Epoch 144/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.5199 - accuracy: 0.7361 - val_loss: 0.6144 - val_accuracy: 0.6775\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.50788\n",
            "Epoch 145/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5124 - accuracy: 0.7407 - val_loss: 0.6179 - val_accuracy: 0.6703\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.50788\n",
            "Epoch 146/300\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.5166 - accuracy: 0.7358 - val_loss: 0.6192 - val_accuracy: 0.6740\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.50788\n",
            "Epoch 147/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.5145 - accuracy: 0.7380 - val_loss: 0.6174 - val_accuracy: 0.6675\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.50788\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.5093 - accuracy: 0.7406 - val_loss: 0.6161 - val_accuracy: 0.6770\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.50788\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.5028 - accuracy: 0.7466 - val_loss: 0.6167 - val_accuracy: 0.6803\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.50788\n",
            "Epoch 150/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4952 - accuracy: 0.7536 - val_loss: 0.6189 - val_accuracy: 0.6878\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.50788\n",
            "Epoch 151/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4985 - accuracy: 0.7516 - val_loss: 0.6186 - val_accuracy: 0.6843\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.50788\n",
            "Epoch 152/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.4921 - accuracy: 0.7594 - val_loss: 0.6241 - val_accuracy: 0.6833\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.50788\n",
            "Epoch 153/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4858 - accuracy: 0.7598 - val_loss: 0.6166 - val_accuracy: 0.6898\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.50788\n",
            "Epoch 154/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4767 - accuracy: 0.7684 - val_loss: 0.6132 - val_accuracy: 0.6883\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.50788\n",
            "Epoch 155/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.4668 - accuracy: 0.7762 - val_loss: 0.6164 - val_accuracy: 0.6913\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.50788\n",
            "Epoch 156/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.4593 - accuracy: 0.7815 - val_loss: 0.6167 - val_accuracy: 0.6930\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.50788\n",
            "Epoch 157/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4559 - accuracy: 0.7833 - val_loss: 0.6227 - val_accuracy: 0.6933\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.50788\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4532 - accuracy: 0.7826 - val_loss: 0.6229 - val_accuracy: 0.6863\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.50788\n",
            "Epoch 159/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4473 - accuracy: 0.7896 - val_loss: 0.6268 - val_accuracy: 0.6938\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.50788\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4544 - accuracy: 0.7855 - val_loss: 0.6243 - val_accuracy: 0.6973\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.50788\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.4585 - accuracy: 0.7823 - val_loss: 0.6287 - val_accuracy: 0.6915\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.50788\n",
            "Epoch 162/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4662 - accuracy: 0.7758 - val_loss: 0.6259 - val_accuracy: 0.6958\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50788\n",
            "Epoch 163/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4731 - accuracy: 0.7684 - val_loss: 0.6231 - val_accuracy: 0.6970\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.50788\n",
            "Epoch 164/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4609 - accuracy: 0.7779 - val_loss: 0.6308 - val_accuracy: 0.7053\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.50788\n",
            "Epoch 165/300\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 0.4441 - accuracy: 0.7921 - val_loss: 0.6231 - val_accuracy: 0.7003\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.50788\n",
            "Epoch 166/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4320 - accuracy: 0.7978 - val_loss: 0.6261 - val_accuracy: 0.7023\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.50788\n",
            "Epoch 167/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4251 - accuracy: 0.8031 - val_loss: 0.6394 - val_accuracy: 0.7060\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.50788\n",
            "Epoch 168/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4264 - accuracy: 0.8017 - val_loss: 0.6427 - val_accuracy: 0.7075\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.50788\n",
            "Epoch 169/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.4231 - accuracy: 0.8047 - val_loss: 0.6362 - val_accuracy: 0.7055\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.50788\n",
            "Epoch 170/300\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 0.4140 - accuracy: 0.8091 - val_loss: 0.6407 - val_accuracy: 0.7003\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.50788\n",
            "Epoch 171/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4118 - accuracy: 0.8116 - val_loss: 0.6456 - val_accuracy: 0.6983\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.50788\n",
            "Epoch 172/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4094 - accuracy: 0.8141 - val_loss: 0.6479 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.50788\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4088 - accuracy: 0.8148 - val_loss: 0.6495 - val_accuracy: 0.6940\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.50788\n",
            "Epoch 174/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4055 - accuracy: 0.8149 - val_loss: 0.6556 - val_accuracy: 0.7030\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.50788\n",
            "Epoch 175/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4058 - accuracy: 0.8151 - val_loss: 0.6515 - val_accuracy: 0.7038\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.50788\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4179 - accuracy: 0.8082 - val_loss: 0.6563 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.50788\n",
            "Epoch 177/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.4248 - accuracy: 0.8043 - val_loss: 0.6562 - val_accuracy: 0.7028\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.50788\n",
            "Epoch 178/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.4084 - accuracy: 0.8153 - val_loss: 0.6511 - val_accuracy: 0.7015\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.50788\n",
            "Epoch 179/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3982 - accuracy: 0.8193 - val_loss: 0.6542 - val_accuracy: 0.7015\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.50788\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.3874 - accuracy: 0.8253 - val_loss: 0.6618 - val_accuracy: 0.7018\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.50788\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3859 - accuracy: 0.8255 - val_loss: 0.6646 - val_accuracy: 0.7015\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.50788\n",
            "Epoch 182/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3995 - accuracy: 0.8175 - val_loss: 0.6706 - val_accuracy: 0.6965\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.50788\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3908 - accuracy: 0.8238 - val_loss: 0.6686 - val_accuracy: 0.6995\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.50788\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3845 - accuracy: 0.8273 - val_loss: 0.6643 - val_accuracy: 0.7003\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.50788\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3768 - accuracy: 0.8299 - val_loss: 0.6673 - val_accuracy: 0.6998\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.50788\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.3668 - accuracy: 0.8375 - val_loss: 0.6676 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.50788\n",
            "Epoch 187/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3720 - accuracy: 0.8346 - val_loss: 0.6763 - val_accuracy: 0.6918\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.50788\n",
            "Epoch 188/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3623 - accuracy: 0.8399 - val_loss: 0.6748 - val_accuracy: 0.7020\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.50788\n",
            "Epoch 189/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.3583 - accuracy: 0.8408 - val_loss: 0.6743 - val_accuracy: 0.7013\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.50788\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3645 - accuracy: 0.8373 - val_loss: 0.6849 - val_accuracy: 0.6993\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.50788\n",
            "Epoch 191/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3600 - accuracy: 0.8399 - val_loss: 0.6884 - val_accuracy: 0.6960\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.50788\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.3546 - accuracy: 0.8461 - val_loss: 0.6909 - val_accuracy: 0.6943\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.50788\n",
            "Epoch 193/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3594 - accuracy: 0.8411 - val_loss: 0.7036 - val_accuracy: 0.6933\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.50788\n",
            "Epoch 194/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.3589 - accuracy: 0.8438 - val_loss: 0.7021 - val_accuracy: 0.6965\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.50788\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.3590 - accuracy: 0.8411 - val_loss: 0.7035 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.50788\n",
            "Epoch 196/300\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.3559 - accuracy: 0.8439 - val_loss: 0.7050 - val_accuracy: 0.6915\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.50788\n",
            "Epoch 197/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3583 - accuracy: 0.8438 - val_loss: 0.7137 - val_accuracy: 0.6858\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.50788\n",
            "Epoch 198/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3706 - accuracy: 0.8331 - val_loss: 0.7180 - val_accuracy: 0.6920\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.50788\n",
            "Epoch 199/300\n",
            "16/16 [==============================] - 1s 57ms/step - loss: 0.3595 - accuracy: 0.8402 - val_loss: 0.7053 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.50788\n",
            "Epoch 200/300\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.3579 - accuracy: 0.8401 - val_loss: 0.7078 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.50788\n",
            "Epoch 201/300\n",
            "16/16 [==============================] - 1s 60ms/step - loss: 0.3631 - accuracy: 0.8372 - val_loss: 0.7235 - val_accuracy: 0.6993\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.50788\n",
            "Epoch 202/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3574 - accuracy: 0.8423 - val_loss: 0.7133 - val_accuracy: 0.6985\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.50788\n",
            "Epoch 203/300\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.3558 - accuracy: 0.8422 - val_loss: 0.7064 - val_accuracy: 0.6905\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.50788\n",
            "Epoch 204/300\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.3548 - accuracy: 0.8406 - val_loss: 0.7173 - val_accuracy: 0.6983\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.50788\n",
            "Epoch 205/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3513 - accuracy: 0.8464 - val_loss: 0.7102 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.50788\n",
            "Epoch 206/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3372 - accuracy: 0.8538 - val_loss: 0.7135 - val_accuracy: 0.7058\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.50788\n",
            "Epoch 207/300\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.3246 - accuracy: 0.8618 - val_loss: 0.7142 - val_accuracy: 0.7058\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.50788\n",
            "Epoch 208/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3140 - accuracy: 0.8680 - val_loss: 0.7220 - val_accuracy: 0.6983\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.50788\n",
            "Epoch 209/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3171 - accuracy: 0.8680 - val_loss: 0.7348 - val_accuracy: 0.7013\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.50788\n",
            "Epoch 210/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3147 - accuracy: 0.8673 - val_loss: 0.7340 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.50788\n",
            "Epoch 211/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3523 - accuracy: 0.8426 - val_loss: 0.7341 - val_accuracy: 0.6978\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.50788\n",
            "Epoch 212/300\n",
            "16/16 [==============================] - 2s 122ms/step - loss: 0.3520 - accuracy: 0.8447 - val_loss: 0.7292 - val_accuracy: 0.7080\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.50788\n",
            "Epoch 213/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3352 - accuracy: 0.8550 - val_loss: 0.7281 - val_accuracy: 0.7035\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.50788\n",
            "Epoch 214/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.3422 - accuracy: 0.8521 - val_loss: 0.7472 - val_accuracy: 0.7043\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.50788\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.3467 - accuracy: 0.8480 - val_loss: 0.7209 - val_accuracy: 0.7025\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.50788\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3383 - accuracy: 0.8526 - val_loss: 0.7422 - val_accuracy: 0.7063\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.50788\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3490 - accuracy: 0.8499 - val_loss: 0.7419 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.50788\n",
            "Epoch 218/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3343 - accuracy: 0.8583 - val_loss: 0.7325 - val_accuracy: 0.7010\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.50788\n",
            "Epoch 219/300\n",
            "16/16 [==============================] - 1s 59ms/step - loss: 0.3557 - accuracy: 0.8469 - val_loss: 0.7346 - val_accuracy: 0.7093\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.50788\n",
            "Epoch 220/300\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.3677 - accuracy: 0.8383 - val_loss: 0.7349 - val_accuracy: 0.7065\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.50788\n",
            "Epoch 221/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4017 - accuracy: 0.8183 - val_loss: 0.7415 - val_accuracy: 0.6845\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.50788\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3902 - accuracy: 0.8223 - val_loss: 0.7417 - val_accuracy: 0.6918\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.50788\n",
            "Epoch 223/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.3723 - accuracy: 0.8340 - val_loss: 0.7249 - val_accuracy: 0.6953\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.50788\n",
            "Epoch 224/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3605 - accuracy: 0.8398 - val_loss: 0.7299 - val_accuracy: 0.7035\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.50788\n",
            "Epoch 225/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3641 - accuracy: 0.8413 - val_loss: 0.7392 - val_accuracy: 0.6900\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.50788\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3539 - accuracy: 0.8439 - val_loss: 0.7300 - val_accuracy: 0.6890\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.50788\n",
            "Epoch 227/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3867 - accuracy: 0.8246 - val_loss: 0.7716 - val_accuracy: 0.6583\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.50788\n",
            "Epoch 228/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4987 - accuracy: 0.7705 - val_loss: 0.7669 - val_accuracy: 0.6690\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.50788\n",
            "Epoch 229/300\n",
            "16/16 [==============================] - 2s 122ms/step - loss: 0.4597 - accuracy: 0.7886 - val_loss: 0.7217 - val_accuracy: 0.6940\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.50788\n",
            "Epoch 230/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4321 - accuracy: 0.8028 - val_loss: 0.7181 - val_accuracy: 0.6925\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.50788\n",
            "Epoch 231/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4223 - accuracy: 0.8101 - val_loss: 0.7047 - val_accuracy: 0.6935\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.50788\n",
            "Epoch 232/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4038 - accuracy: 0.8193 - val_loss: 0.7004 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.50788\n",
            "Epoch 233/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3892 - accuracy: 0.8265 - val_loss: 0.7043 - val_accuracy: 0.7025\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.50788\n",
            "Epoch 234/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3843 - accuracy: 0.8275 - val_loss: 0.7064 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.50788\n",
            "Epoch 235/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3763 - accuracy: 0.8330 - val_loss: 0.7130 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.50788\n",
            "Epoch 236/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3972 - accuracy: 0.8260 - val_loss: 0.7283 - val_accuracy: 0.7038\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.50788\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.4000 - accuracy: 0.8223 - val_loss: 0.7112 - val_accuracy: 0.7095\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.50788\n",
            "Epoch 238/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3865 - accuracy: 0.8300 - val_loss: 0.7055 - val_accuracy: 0.7128\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.50788\n",
            "Epoch 239/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3778 - accuracy: 0.8336 - val_loss: 0.6992 - val_accuracy: 0.7058\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.50788\n",
            "Epoch 240/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3609 - accuracy: 0.8451 - val_loss: 0.7001 - val_accuracy: 0.7095\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.50788\n",
            "Epoch 241/300\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 0.3589 - accuracy: 0.8448 - val_loss: 0.7036 - val_accuracy: 0.7033\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.50788\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 0.3625 - accuracy: 0.8398 - val_loss: 0.7101 - val_accuracy: 0.7113\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.50788\n",
            "Epoch 243/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3544 - accuracy: 0.8459 - val_loss: 0.7471 - val_accuracy: 0.7093\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.50788\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4371 - accuracy: 0.8140 - val_loss: 0.7204 - val_accuracy: 0.7045\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.50788\n",
            "Epoch 245/300\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.4204 - accuracy: 0.8134 - val_loss: 0.7028 - val_accuracy: 0.6933\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.50788\n",
            "Epoch 246/300\n",
            "16/16 [==============================] - 2s 130ms/step - loss: 0.3983 - accuracy: 0.8239 - val_loss: 0.7036 - val_accuracy: 0.7105\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.50788\n",
            "Epoch 247/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.4004 - accuracy: 0.8224 - val_loss: 0.6925 - val_accuracy: 0.7065\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.50788\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.3996 - accuracy: 0.8197 - val_loss: 0.6910 - val_accuracy: 0.7050\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.50788\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3947 - accuracy: 0.8205 - val_loss: 0.6986 - val_accuracy: 0.7118\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.50788\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3893 - accuracy: 0.8239 - val_loss: 0.6891 - val_accuracy: 0.7075\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.50788\n",
            "Epoch 251/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3687 - accuracy: 0.8349 - val_loss: 0.6905 - val_accuracy: 0.7110\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.50788\n",
            "Epoch 252/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3711 - accuracy: 0.8365 - val_loss: 0.7022 - val_accuracy: 0.7115\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.50788\n",
            "Epoch 253/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.5063 - accuracy: 0.7693 - val_loss: 0.7202 - val_accuracy: 0.6603\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.50788\n",
            "Epoch 254/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.5153 - accuracy: 0.7491 - val_loss: 0.6964 - val_accuracy: 0.6790\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.50788\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.4875 - accuracy: 0.7658 - val_loss: 0.6817 - val_accuracy: 0.6898\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.50788\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4643 - accuracy: 0.7801 - val_loss: 0.6816 - val_accuracy: 0.6890\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.50788\n",
            "Epoch 257/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4512 - accuracy: 0.7908 - val_loss: 0.6767 - val_accuracy: 0.6920\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.50788\n",
            "Epoch 258/300\n",
            "16/16 [==============================] - 1s 61ms/step - loss: 0.4400 - accuracy: 0.7962 - val_loss: 0.6741 - val_accuracy: 0.6958\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.50788\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.4392 - accuracy: 0.7950 - val_loss: 0.6721 - val_accuracy: 0.6948\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.50788\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4265 - accuracy: 0.8028 - val_loss: 0.6763 - val_accuracy: 0.6988\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.50788\n",
            "Epoch 261/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4247 - accuracy: 0.8027 - val_loss: 0.6759 - val_accuracy: 0.6978\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.50788\n",
            "Epoch 262/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.4375 - accuracy: 0.7978 - val_loss: 0.6761 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.50788\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.4255 - accuracy: 0.8023 - val_loss: 0.6778 - val_accuracy: 0.6930\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.50788\n",
            "Epoch 264/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4137 - accuracy: 0.8116 - val_loss: 0.6776 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.50788\n",
            "Epoch 265/300\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 0.4040 - accuracy: 0.8184 - val_loss: 0.6758 - val_accuracy: 0.6970\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.50788\n",
            "Epoch 266/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3951 - accuracy: 0.8221 - val_loss: 0.6779 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.50788\n",
            "Epoch 267/300\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 0.3874 - accuracy: 0.8268 - val_loss: 0.6839 - val_accuracy: 0.6915\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.50788\n",
            "Epoch 268/300\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.3832 - accuracy: 0.8310 - val_loss: 0.6866 - val_accuracy: 0.6865\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.50788\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3787 - accuracy: 0.8327 - val_loss: 0.6927 - val_accuracy: 0.6940\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.50788\n",
            "Epoch 270/300\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.3702 - accuracy: 0.8390 - val_loss: 0.6936 - val_accuracy: 0.6860\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.50788\n",
            "Epoch 271/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3634 - accuracy: 0.8433 - val_loss: 0.6995 - val_accuracy: 0.6903\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.50788\n",
            "Epoch 272/300\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 0.3576 - accuracy: 0.8471 - val_loss: 0.7013 - val_accuracy: 0.6923\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.50788\n",
            "Epoch 273/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3563 - accuracy: 0.8466 - val_loss: 0.7049 - val_accuracy: 0.6858\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.50788\n",
            "Epoch 274/300\n",
            "16/16 [==============================] - 1s 67ms/step - loss: 0.3545 - accuracy: 0.8468 - val_loss: 0.7135 - val_accuracy: 0.6893\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.50788\n",
            "Epoch 275/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3478 - accuracy: 0.8517 - val_loss: 0.7125 - val_accuracy: 0.6915\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.50788\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 0.3422 - accuracy: 0.8546 - val_loss: 0.7137 - val_accuracy: 0.6900\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.50788\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - 1s 68ms/step - loss: 0.3376 - accuracy: 0.8571 - val_loss: 0.7194 - val_accuracy: 0.6900\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.50788\n",
            "Epoch 278/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3338 - accuracy: 0.8601 - val_loss: 0.7258 - val_accuracy: 0.6885\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.50788\n",
            "Epoch 279/300\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 0.3397 - accuracy: 0.8534 - val_loss: 0.7358 - val_accuracy: 0.6893\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.50788\n",
            "Epoch 280/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3392 - accuracy: 0.8538 - val_loss: 0.7324 - val_accuracy: 0.6858\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.50788\n",
            "Epoch 281/300\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.3328 - accuracy: 0.8565 - val_loss: 0.7342 - val_accuracy: 0.6840\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.50788\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3289 - accuracy: 0.8601 - val_loss: 0.7376 - val_accuracy: 0.6878\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.50788\n",
            "Epoch 283/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3211 - accuracy: 0.8654 - val_loss: 0.7429 - val_accuracy: 0.6855\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.50788\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 0.3165 - accuracy: 0.8674 - val_loss: 0.7399 - val_accuracy: 0.6870\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.50788\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4117 - accuracy: 0.8154 - val_loss: 0.7615 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.50788\n",
            "Epoch 286/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.4310 - accuracy: 0.8075 - val_loss: 0.7449 - val_accuracy: 0.6915\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.50788\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.4164 - accuracy: 0.8107 - val_loss: 0.7278 - val_accuracy: 0.6988\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.50788\n",
            "Epoch 288/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4029 - accuracy: 0.8187 - val_loss: 0.7285 - val_accuracy: 0.6900\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.50788\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3981 - accuracy: 0.8211 - val_loss: 0.7353 - val_accuracy: 0.6895\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.50788\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.3863 - accuracy: 0.8264 - val_loss: 0.7266 - val_accuracy: 0.6965\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.50788\n",
            "Epoch 291/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3763 - accuracy: 0.8339 - val_loss: 0.7229 - val_accuracy: 0.6955\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.50788\n",
            "Epoch 292/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3693 - accuracy: 0.8373 - val_loss: 0.7184 - val_accuracy: 0.6980\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.50788\n",
            "Epoch 293/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3632 - accuracy: 0.8405 - val_loss: 0.7155 - val_accuracy: 0.6963\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.50788\n",
            "Epoch 294/300\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3724 - accuracy: 0.8338 - val_loss: 0.7210 - val_accuracy: 0.7058\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.50788\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4155 - accuracy: 0.8143 - val_loss: 0.7191 - val_accuracy: 0.6943\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.50788\n",
            "Epoch 296/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.4016 - accuracy: 0.8203 - val_loss: 0.7151 - val_accuracy: 0.7018\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.50788\n",
            "Epoch 297/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3848 - accuracy: 0.8298 - val_loss: 0.7108 - val_accuracy: 0.7048\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.50788\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - 1s 65ms/step - loss: 0.3769 - accuracy: 0.8330 - val_loss: 0.7053 - val_accuracy: 0.6940\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.50788\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.3660 - accuracy: 0.8391 - val_loss: 0.7088 - val_accuracy: 0.7060\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.50788\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.3547 - accuracy: 0.8463 - val_loss: 0.7140 - val_accuracy: 0.7093\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.50788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrFNwQGCTwPf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "bead4206-8338-4ada-a51a-8dcde3de23df"
      },
      "source": [
        "# 훈련 결과를 확인\n",
        "plt.plot(history.epoch, history.history['accuracy'], '-o', label='accuracy')\n",
        "plt.plot(history.epoch, history.history['val_accuracy'], '-o', label='val_accuracy')\n",
        "plt.legend()\n",
        "plt.xlim(left=0)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGWJ1zA2ytA"
      },
      "source": [
        "\"학습 기록장\"\n",
        "\n",
        "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "## batch_size = 10, epochs = 8 =>loss:  0.5526 - accuracy: 0.7073\n",
        "## batch_size = 20, epochs = 4 => loss: 0.4978 - accuracy: 0.7573\n",
        "## batch_size = 20, epochs = 8 => loss: 0.4747 - accuracy: 0.7741\n",
        "## batch_size = 20, epochs = 20=> loss: 0.5539 - accuracy: 0.7261 //50s\n",
        "## batch_size = 32. epochs = 10=> 9/10 : loss: 0.4586 - accuracy: 0.7797, 10/10 : loss: 0.5709 - accuracy: 0.6973\n",
        "## batch_size = 64. epochs = 4 => loss: 0.5084 - accuracy: 0.7492\n",
        "## batch_size = 64. epochs = 8 => loss: 0.5062 - accuracy: 0.7529\n",
        "## batch_size = no. epochs = 9 => loss: 0.5385 - accuracy: 0.7337\n",
        "## batch_size = 110, epochs = 20 => loss: 0.2075 - accuracy: 0.9121 - val_loss: 0.9605 - val_accuracy: 0.7003 //18s overfitting epochs 5 best\n",
        "## batch_size = 120, epochs = 5  => loss: 0.4071 - accuracy: 0.8126 - val_loss: 0.5647 - val_accuracy: 0.7135 //17s overfitting epochs 2 best\n",
        "## batch_size = 120, epochs = 20 => loss: 0.2212 - accuracy: 0.9061 - val_loss: 0.9623 - val_accuracy: 0.7232 //15s overfitting\n",
        "## batch_size = 200, epochs = 30 => loss: 0.3781 - accuracy: 0.8256 - val_loss: 0.5760 - val_accuracy: 0.7368 //16s overfitting epochs 20 best\n",
        "## batch_size = 200, epochs = 20 => loss: 0.2351 - accuracy: 0.8979 - val_loss: 0.9516 - val_accuracy: 0.7185 //16s overfitting epochs 4 best\n",
        "#model.compile(loss='binary_crossentropy',optimizer=adam(learning_rate=0.1, beta_1=0.99, beta_2=0.99, epsilon=0.00001, decay=0.0, amsgrad=False),metrics=['accuracy'])\n",
        "## batch_size = 20, epochs = 4 => loss: 1.0007 - accuracy: 0.5050\n",
        "#model.compile(loss='binary_crossentropy',optimizer=adam(beta_1=0.99),metrics=['accuracy'])\n",
        "## batch_size = 20, epochs = 4 => loss: 0.5551 - accuracy: 0.7246\n",
        "## batch_size = 20, epochs = 8 => loss: 0.5563 - accuracy: 0.7134\n",
        "#model.compile(loss='binary_crossentropy',optimizer=adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999,amsgrad=False),metrics=['accuracy'])\n",
        "## batch_size = 20, epochs = 4 => loss: 0.5014 - accuracy: 0.7529\n",
        "#model.compile(loss='binary_crossentropy',optimizer=adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.01),metrics=['accuracy'])\n",
        "## batch_size = 120, epochs = 30 => loss: 0.3883 - accuracy: 0.8248 - val_loss: 0.5389 - val_accuracy: 0.7400\n",
        "#model.compile(loss='binary_crossentropy',optimizer=adam(learning_rate=0.1, beta_1=0.9, beta_2=0.99, decay=0.01),metrics=['accuracy'])\n",
        "## batch_size = 120, epochs = 50 => loss: 0.5610 - accuracy: 0.6850 - val_loss: 0.7416 - val_accuracy: 0.5410 //15s underfitting\n",
        "## batch_size = 300, epochs = 5  => loss: 0.6962 - accuracy: 0.5366 - val_loss: 0.7145 - val_accuracy: 0.4980 //12s underfitting\n",
        "## batch_size = 500, epochs = 5  => loss: 0.7006 - accuracy: 0.5210 - val_loss: 0.7004 - val_accuracy: 0.5205 //12s underfitting\n",
        "## batch_size = 1000, epochs = 5 => loss: 0.6795 - accuracy: 0.5429 - val_loss: 0.6792 - val_accuracy: 0.5395 //10s underfitting\n",
        "## batch_size = 2000, epochs = 5 => loss: 0.7091 - accuracy: 0.5132 - val_loss: 0.7091 - val_accuracy: 0.4902 //12s underfitting\n",
        "## batch_size = 1000, epochs = 50 => loss: 0.5210 - accuracy: 0.7422 - val_loss: 0.5430 - val_accuracy: 0.7335\n",
        "## batch_size = 1000, epochs = 100 => loss: 0.5506 - accuracy: 0.7003 - val_loss: 0.6662 - val_accuracy: 0.6245"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP7GxF2NxBM6",
        "outputId": "e272e93d-9619-47d5-8747-f7bc201a294d"
      },
      "source": [
        "#가장 좋은 모델의 가중치 불러오기\n",
        "checkpoint_path = pth.join(MODEL_PATH, MODEL_NAME)\n",
        "weigth_file = glob('{}/*.hdf5'.format(checkpoint_path))[-1]\n",
        "print(weigth_file)\n",
        "model.load_weights(weigth_file)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ai_project/RNN_project_data/RNN_test/Epoch_017_Val_0.508.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOC7KA2-5yhh",
        "outputId": "7b6d6473-4777-4854-de38-ef4f0be22de1"
      },
      "source": [
        "vindos"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'아': 2660,\n",
              " '더빙': 64,\n",
              " '.': 15368,\n",
              " '진짜': 1051,\n",
              " '짜증': 190,\n",
              " '나': 2043,\n",
              " '네요': 983,\n",
              " '목소리': 37,\n",
              " '흠': 40,\n",
              " '...': 3392,\n",
              " '포스터': 85,\n",
              " '보고': 195,\n",
              " '초딩': 50,\n",
              " '영화': 7068,\n",
              " '줄': 246,\n",
              " '....': 286,\n",
              " '오버': 16,\n",
              " '연기': 962,\n",
              " '조차': 41,\n",
              " '가볍': 48,\n",
              " '지': 2719,\n",
              " '않': 968,\n",
              " '구나': 90,\n",
              " '너무재밓었다그래서보는것을추천한다': 1,\n",
              " '교도소': 3,\n",
              " '이야기': 272,\n",
              " '이': 13301,\n",
              " '구먼': 5,\n",
              " '..': 1076,\n",
              " '솔직히': 148,\n",
              " '재미': 500,\n",
              " '는': 8723,\n",
              " '없': 2083,\n",
              " '다': 7089,\n",
              " '평점': 836,\n",
              " '조정': 6,\n",
              " '사이몬페그의': 1,\n",
              " '익살': 3,\n",
              " '스럽': 330,\n",
              " 'ㄴ': 9641,\n",
              " '가': 4277,\n",
              " '돋보이': 43,\n",
              " '었': 3287,\n",
              " '던': 757,\n",
              " '!': 1576,\n",
              " '스파이더맨': 15,\n",
              " '에서': 1050,\n",
              " '늙': 18,\n",
              " '어': 2931,\n",
              " '보이': 360,\n",
              " '기': 1285,\n",
              " '만': 1591,\n",
              " '하': 12721,\n",
              " '았': 3940,\n",
              " '커스틴 던스트': 1,\n",
              " '너무나': 94,\n",
              " '도': 4193,\n",
              " '이뻐보였다': 1,\n",
              " '막': 68,\n",
              " '걸음마': 1,\n",
              " '떼': 30,\n",
              " '3': 299,\n",
              " '세': 162,\n",
              " '부터': 261,\n",
              " '초등학교': 17,\n",
              " '1': 712,\n",
              " '학년': 18,\n",
              " '생': 67,\n",
              " '8살용영화.ㅋㅋㅋ...별반개도': 1,\n",
              " '아깝': 597,\n",
              " 'ㅁ': 2089,\n",
              " '원작': 168,\n",
              " '의': 4270,\n",
              " '긴장감': 115,\n",
              " '을': 4085,\n",
              " '제대로': 96,\n",
              " '살리': 61,\n",
              " '내': 1070,\n",
              " '못하': 403,\n",
              " '별': 204,\n",
              " '반개': 30,\n",
              " '욕': 127,\n",
              " '나오': 903,\n",
              " 'ㄴ다': 1097,\n",
              " '이응경': 1,\n",
              " '길용우': 1,\n",
              " '생활': 12,\n",
              " '몇': 148,\n",
              " '년': 260,\n",
              " 'ㄴ지': 237,\n",
              " '정말': 1266,\n",
              " '발로': 12,\n",
              " '아도': 474,\n",
              " '그것': 97,\n",
              " '보다': 646,\n",
              " '낫겟다': 1,\n",
              " '납치': 5,\n",
              " '감금': 1,\n",
              " '반복': 23,\n",
              " '드라마': 650,\n",
              " '가족': 131,\n",
              " '사람': 673,\n",
              " '모이': 6,\n",
              " '엇': 191,\n",
              " '네': 998,\n",
              " '액션': 336,\n",
              " '는데': 1547,\n",
              " '있': 2101,\n",
              " '안': 1116,\n",
              " '되': 1751,\n",
              " '왜': 800,\n",
              " '게': 3703,\n",
              " '낮': 180,\n",
              " '은': 4010,\n",
              " '것': 1424,\n",
              " 'ㄴ데': 420,\n",
              " '?': 2276,\n",
              " '꽤': 65,\n",
              " '보': 6068,\n",
              " 'ㄹ': 3285,\n",
              " '헐리우드': 20,\n",
              " '식': 80,\n",
              " '화려': 56,\n",
              " '에': 4230,\n",
              " '너무': 1350,\n",
              " '길들이': 1,\n",
              " '걍인피니트가짱이다.진짜짱이다♥': 1,\n",
              " '볼': 123,\n",
              " '때': 722,\n",
              " '마다': 85,\n",
              " '눈물': 168,\n",
              " '나서': 49,\n",
              " '죽': 209,\n",
              " '겠': 745,\n",
              " '90': 11,\n",
              " '년대': 54,\n",
              " '향수': 9,\n",
              " '자극': 41,\n",
              " '!!': 638,\n",
              " '허진호': 2,\n",
              " '감성': 54,\n",
              " '절제': 10,\n",
              " '멜로': 44,\n",
              " '달인': 7,\n",
              " '이다': 78,\n",
              " '~': 1714,\n",
              " '울': 180,\n",
              " '면서': 429,\n",
              " '손들': 1,\n",
              " '고': 6058,\n",
              " '횡단보도': 1,\n",
              " '건너': 3,\n",
              " '뛰쳐나오': 4,\n",
              " '뻔': 84,\n",
              " '이범수': 5,\n",
              " '드럽': 26,\n",
              " '담백': 5,\n",
              " '깔끔': 21,\n",
              " '아서': 1038,\n",
              " '좋': 1447,\n",
              " '신문': 1,\n",
              " '기사': 7,\n",
              " '로만': 7,\n",
              " '면': 869,\n",
              " '자꾸': 30,\n",
              " '잊어버리': 4,\n",
              " '그': 735,\n",
              " '들': 2802,\n",
              " '다는': 336,\n",
              " '취향': 34,\n",
              " '존중': 6,\n",
              " 'ㄴ다지만': 3,\n",
              " '극장': 124,\n",
              " '중': 500,\n",
              " '가장': 199,\n",
              " '노': 171,\n",
              " '재': 501,\n",
              " '감동': 632,\n",
              " '스토리': 684,\n",
              " '어거지': 14,\n",
              " 'ㄱ냥': 1,\n",
              " '매번': 9,\n",
              " '긴장': 21,\n",
              " '재밋음ㅠㅠ': 1,\n",
              " '참': 364,\n",
              " '웃기': 186,\n",
              " '바스코': 4,\n",
              " '이기': 44,\n",
              " '락스': 2,\n",
              " '코': 61,\n",
              " '라고': 415,\n",
              " '까': 78,\n",
              " '바비': 4,\n",
              " '아이돌': 20,\n",
              " '깔': 26,\n",
              " '그냥': 547,\n",
              " '싶': 617,\n",
              " '어서': 537,\n",
              " '안달': 1,\n",
              " '난': 32,\n",
              " '처럼': 148,\n",
              " '굿바이 레닌': 1,\n",
              " '표절': 30,\n",
              " '이해': 224,\n",
              " '뒤': 67,\n",
              " '로': 1021,\n",
              " '갈수록': 70,\n",
              " '재미없': 481,\n",
              " '냐': 414,\n",
              " '이건': 353,\n",
              " '깨알': 10,\n",
              " '캐스팅': 84,\n",
              " '과': 942,\n",
              " '질퍽': 1,\n",
              " '산뜻': 2,\n",
              " '내용': 557,\n",
              " '구성': 67,\n",
              " '잘': 795,\n",
              " '버': 24,\n",
              " '무': 93,\n",
              " '러': 87,\n",
              " '진': 21,\n",
              " '드': 85,\n",
              " '♥': 128,\n",
              " '약탈': 1,\n",
              " '자': 281,\n",
              " '를': 2187,\n",
              " '위하': 180,\n",
              " '변명': 4,\n",
              " ',': 2968,\n",
              " '라': 603,\n",
              " '저놈': 3,\n",
              " '착하': 33,\n",
              " '놈': 90,\n",
              " '절대': 97,\n",
              " '아니': 1005,\n",
              " 'ㄴ걸요': 1,\n",
              " '나름': 108,\n",
              " '심오': 11,\n",
              " '뜻': 21,\n",
              " '듯': 481,\n",
              " '학생': 19,\n",
              " '선생': 8,\n",
              " '놀아나': 3,\n",
              " '웃': 154,\n",
              " '건': 390,\n",
              " '불': 60,\n",
              " '가능': 45,\n",
              " '지루': 506,\n",
              " '같': 1097,\n",
              " '음식': 13,\n",
              " '바베트의': 1,\n",
              " '만찬': 2,\n",
              " '넘': 260,\n",
              " '차이남....바베트의': 1,\n",
              " ';': 822,\n",
              " 'ㄹ게': 44,\n",
              " '별로': 276,\n",
              " '핀란드': 2,\n",
              " '풍경': 11,\n",
              " '라도': 109,\n",
              " '구경할랫는데': 1,\n",
              " 'ㅡㅡ': 105,\n",
              " '평범': 40,\n",
              " '수작': 89,\n",
              " '라는': 318,\n",
              " '거': 965,\n",
              " '말씀': 8,\n",
              " '드리': 51,\n",
              " 'ㅂ니다': 804,\n",
              " '주제': 55,\n",
              " '은데': 185,\n",
              " '중반': 28,\n",
              " '짜르': 1,\n",
              " '꺼': 82,\n",
              " '야': 373,\n",
              " '그래서': 53,\n",
              " '납득': 9,\n",
              " '수': 909,\n",
              " '그렇': 282,\n",
              " '꼭': 235,\n",
              " '걸': 204,\n",
              " '끄': 57,\n",
              " '어야': 190,\n",
              " 'kl': 1,\n",
              " '2': 383,\n",
              " 'g': 1,\n",
              " '고추': 5,\n",
              " '털': 9,\n",
              " '버리': 240,\n",
              " 'ㄹ텐데': 16,\n",
              " '카밀라': 2,\n",
              " '벨': 6,\n",
              " '발': 114,\n",
              " '재밋는뎅': 1,\n",
              " '센스': 13,\n",
              " '연출력': 40,\n",
              " '탁월': 11,\n",
              " '90년대': 31,\n",
              " '9': 90,\n",
              " '점': 1169,\n",
              " '엄': 18,\n",
              " '포스': 15,\n",
              " '위력': 4,\n",
              " '다시': 407,\n",
              " '한': 668,\n",
              " '번': 386,\n",
              " '깨닫': 31,\n",
              " '주': 1485,\n",
              " '적': 1276,\n",
              " '남': 394,\n",
              " '꽃': 16,\n",
              " '검사': 7,\n",
              " '님': 205,\n",
              " '어요': 1077,\n",
              " '완전': 316,\n",
              " '명품': 21,\n",
              " '졸': 161,\n",
              " '쓰레기': 434,\n",
              " '진부하고말도안됌ㅋㅋ': 1,\n",
              " '시간': 552,\n",
              " '재밌': 1082,\n",
              " '별점': 76,\n",
              " '이리': 64,\n",
              " '은고': 1,\n",
              " '%': 47,\n",
              " '기대': 316,\n",
              " '죄인': 4,\n",
              " '아직': 166,\n",
              " '인생': 207,\n",
              " '최고': 825,\n",
              " '패션': 6,\n",
              " '대하': 242,\n",
              " '열정': 30,\n",
              " '안나': 30,\n",
              " '윈': 4,\n",
              " '투어': 1,\n",
              " '키이라 나이틀리': 1,\n",
              " '고자': 46,\n",
              " '대체': 65,\n",
              " '정신': 77,\n",
              " '장애': 12,\n",
              " 'ㄹ까': 217,\n",
              " '틱': 9,\n",
              " '허허': 11,\n",
              " '원': 67,\n",
              " '작가': 117,\n",
              " '나가': 78,\n",
              " '유령': 7,\n",
              " '재미있': 503,\n",
              " '관객': 78,\n",
              " '114': 1,\n",
              " '명': 91,\n",
              " '이렇': 512,\n",
              " '저': 231,\n",
              " '평가': 57,\n",
              " '받': 253,\n",
              " '는지': 267,\n",
              " '모르': 428,\n",
              " '단순': 38,\n",
              " '은은': 9,\n",
              " '매력': 193,\n",
              " \"'\": 326,\n",
              " '알바': 88,\n",
              " 'ㄴ가': 435,\n",
              " '무섭': 177,\n",
              " '거도': 5,\n",
              " '하나': 302,\n",
              " '음': 1155,\n",
              " '싱겁': 3,\n",
              " '영화.ㅇ.ㅇ냉시간': 1,\n",
              " '낚이': 47,\n",
              " '오': 529,\n",
              " '두': 255,\n",
              " '어라': 92,\n",
              " '서리': 5,\n",
              " '굶주리': 2,\n",
              " '맘': 71,\n",
              " '또': 269,\n",
              " '방법': 22,\n",
              " '>': 78,\n",
              " 'ㅜㅡ': 2,\n",
              " '윤제문': 1,\n",
              " '멋지': 193,\n",
              " '배우': 600,\n",
              " '발견': 19,\n",
              " '소소': 14,\n",
              " '일탈': 8,\n",
              " '잔잔': 129,\n",
              " '미소': 20,\n",
              " '머금': 4,\n",
              " '음악': 192,\n",
              " '조금': 91,\n",
              " '아쉽네요ㅠㅠ': 1,\n",
              " '8': 85,\n",
              " '올리': 43,\n",
              " '10': 424,\n",
              " 'ㄹ게요': 14,\n",
              " '^': 763,\n",
              " '속지': 3,\n",
              " '마시': 24,\n",
              " '길': 145,\n",
              " '낭비': 64,\n",
              " '돈': 290,\n",
              " '리얼리티': 6,\n",
              " '뛰어나': 35,\n",
              " '크': 184,\n",
              " '공감': 145,\n",
              " '이민기': 9,\n",
              " '캐릭터': 152,\n",
              " '정신의학': 1,\n",
              " '상': 67,\n",
              " '분노': 23,\n",
              " '조절': 23,\n",
              " '초기': 7,\n",
              " '증상': 1,\n",
              " '일': 258,\n",
              " '툭하면': 4,\n",
              " '패': 16,\n",
              " '물건': 6,\n",
              " '파손': 1,\n",
              " '오바': 10,\n",
              " '극': 69,\n",
              " '초반': 85,\n",
              " '신선': 86,\n",
              " '상태': 17,\n",
              " '불가': 22,\n",
              " '마이너스': 20,\n",
              " 'ㅋ': 132,\n",
              " '뮤비': 1,\n",
              " '수준': 170,\n",
              " '딱': 114,\n",
              " '알': 605,\n",
              " '더군': 6,\n",
              " 'ㅉㅉ': 35,\n",
              " '북한': 24,\n",
              " '이런': 719,\n",
              " '만들': 1042,\n",
              " '대': 230,\n",
              " '주더': 2,\n",
              " '우리': 147,\n",
              " '사랑합니다': 12,\n",
              " '데': 224,\n",
              " '너': 59,\n",
              " '리스': 4,\n",
              " '타르': 2,\n",
              " '가르': 3,\n",
              " '용의': 3,\n",
              " '주인': 7,\n",
              " '누': 26,\n",
              " '이랑': 130,\n",
              " '근친상간': 1,\n",
              " '이나': 258,\n",
              " '다니': 196,\n",
              " 'ㄹ지라도': 6,\n",
              " '소설': 63,\n",
              " '속': 182,\n",
              " '제일': 87,\n",
              " '자이': 2,\n",
              " '메': 62,\n",
              " '니스': 7,\n",
              " '터': 24,\n",
              " '라마속': 1,\n",
              " '에선': 29,\n",
              " '드래곤': 1,\n",
              " '(': 192,\n",
              " '용': 177,\n",
              " ')': 211,\n",
              " '웃음': 80,\n",
              " '감독': 564,\n",
              " '토르': 3,\n",
              " '-2': 2,\n",
              " '다크 월드': 1,\n",
              " '말': 952,\n",
              " '잡수': 1,\n",
              " '시': 642,\n",
              " '을지라도': 1,\n",
              " '기본': 28,\n",
              " '선방': 1,\n",
              " '영혼': 11,\n",
              " '어루만지': 3,\n",
              " '군요': 56,\n",
              " '거치': 6,\n",
              " '세상사': 2,\n",
              " '잠시': 17,\n",
              " '잊': 68,\n",
              " '동화': 37,\n",
              " '행복': 80,\n",
              " '르': 22,\n",
              " '게이': 34,\n",
              " '작': 240,\n",
              " '매운맛': 2,\n",
              " '마': 72,\n",
              " '포퐁저그': 1,\n",
              " '콩': 18,\n",
              " '진호': 1,\n",
              " '간다': 16,\n",
              " '가슴': 191,\n",
              " '시리': 6,\n",
              " '을까': 188,\n",
              " '자체': 146,\n",
              " '꼬마': 11,\n",
              " '애가': 16,\n",
              " '무슨': 206,\n",
              " '원한': 2,\n",
              " '깊': 111,\n",
              " '길래': 53,\n",
              " '더니': 57,\n",
              " 'OO': 135,\n",
              " '혼자': 49,\n",
              " '나대': 3,\n",
              " '어쩌': 58,\n",
              " '전': 227,\n",
              " '아요': 247,\n",
              " '충격': 59,\n",
              " '기분': 116,\n",
              " '완전히': 23,\n",
              " '푹': 21,\n",
              " '꺼지': 9,\n",
              " '느낌': 329,\n",
              " '활력': 1,\n",
              " '무겁': 22,\n",
              " '지독': 8,\n",
              " '차갑': 6,\n",
              " '자비': 3,\n",
              " '그저': 89,\n",
              " '일본인': 11,\n",
              " '상상력': 20,\n",
              " '대단': 123,\n",
              " '생각': 656,\n",
              " '심심': 35,\n",
              " '백봉기': 1,\n",
              " '언제나': 22,\n",
              " '나요': 74,\n",
              " '그대로': 65,\n",
              " '들어맞': 2,\n",
              " '예측': 8,\n",
              " '카리스마': 16,\n",
              " '악역': 26,\n",
              " '불알': 1,\n",
              " '당황': 16,\n",
              " '아무튼': 7,\n",
              " '중간': 126,\n",
              " '끝나': 172,\n",
              " '함속': 5,\n",
              " '녹': 15,\n",
              " '아든': 2,\n",
              " '일상': 25,\n",
              " '밋밋': 27,\n",
              " '계속': 126,\n",
              " '전개도': 17,\n",
              " '느리': 25,\n",
              " '주인공': 247,\n",
              " '은희': 7,\n",
              " '한두': 5,\n",
              " '컷': 20,\n",
              " '소극': 1,\n",
              " '모습': 130,\n",
              " 'ㅜㅜ': 44,\n",
              " '맨날': 16,\n",
              " '언제': 67,\n",
              " '전개': 170,\n",
              " '좀': 501,\n",
              " '빨리빨리': 3,\n",
              " '사랑': 370,\n",
              " '가슴속': 5,\n",
              " '감정': 72,\n",
              " '헤집': 1,\n",
              " '놓': 205,\n",
              " '예': 53,\n",
              " '요정': 5,\n",
              " '많': 324,\n",
              " '큐': 70,\n",
              " '우리나라': 84,\n",
              " '슬프': 152,\n",
              " '현대사': 2,\n",
              " '단면': 3,\n",
              " '깊이': 36,\n",
              " '사죄': 2,\n",
              " '바로': 44,\n",
              " '잡기': 2,\n",
              " '노력': 44,\n",
              " '으면': 344,\n",
              " '듣': 37,\n",
              " '보도': 3,\n",
              " '연맹': 1,\n",
              " '민간인': 5,\n",
              " '학살': 9,\n",
              " '이정': 40,\n",
              " '명백': 2,\n",
              " '살인': 31,\n",
              " '살인자': 7,\n",
              " '어디': 149,\n",
              " '예전': 60,\n",
              " '작품': 407,\n",
              " '에피소드': 19,\n",
              " '재탕': 10,\n",
              " '삼탕': 2,\n",
              " '사골': 4,\n",
              " '우려': 8,\n",
              " '먹': 127,\n",
              " '산': 45,\n",
              " '으로': 1167,\n",
              " '시청률': 40,\n",
              " '아예': 13,\n",
              " '이제': 147,\n",
              " '70': 7,\n",
              " '회': 63,\n",
              " '120': 7,\n",
              " '부작': 22,\n",
              " '라니': 80,\n",
              " '김남길': 3,\n",
              " '백': 23,\n",
              " '짜리': 52,\n",
              " '연기력': 122,\n",
              " '몰입': 190,\n",
              " '불구': 13,\n",
              " '손예진': 8,\n",
              " 'ㅈㅈ': 1,\n",
              " '비슷': 57,\n",
              " '안보': 79,\n",
              " '신': 90,\n",
              " '분': 249,\n",
              " '한테': 103,\n",
              " '노래': 100,\n",
              " '실력': 20,\n",
              " '뽑': 17,\n",
              " '맞': 173,\n",
              " '박시환': 3,\n",
              " 'mama': 1,\n",
              " '가면': 7,\n",
              " '망신': 2,\n",
              " '일본': 198,\n",
              " '유치': 199,\n",
              " '이틀': 4,\n",
              " '근데': 113,\n",
              " '차': 75,\n",
              " '넣': 37,\n",
              " '조작': 15,\n",
              " '려고': 102,\n",
              " '열리': 7,\n",
              " '다던지': 1,\n",
              " '집': 69,\n",
              " '활짝': 2,\n",
              " '아무': 49,\n",
              " '들어가': 36,\n",
              " 'ㄴ다던가': 1,\n",
              " '문자': 2,\n",
              " '비번': 2,\n",
              " '걸리': 48,\n",
              " 'ㅋㅋㅋ': 154,\n",
              " '그런': 104,\n",
              " '억지': 90,\n",
              " '그래도': 102,\n",
              " '재밋네요': 1,\n",
              " '달팽이': 3,\n",
              " '빠르': 24,\n",
              " '더': 607,\n",
              " '어설프': 75,\n",
              " '어이없': 83,\n",
              " '결말': 204,\n",
              " '부패': 7,\n",
              " '로마노프 왕조': 1,\n",
              " '기리': 4,\n",
              " '뭣': 15,\n",
              " '온몸': 4,\n",
              " '항거': 1,\n",
              " '러시아': 10,\n",
              " '민중': 5,\n",
              " '폭': 32,\n",
              " '돌': 32,\n",
              " '무난': 9,\n",
              " '편이': 1,\n",
              " '구': 117,\n",
              " '습니다': 680,\n",
              " '매우': 67,\n",
              " '실망': 174,\n",
              " '한국': 226,\n",
              " '흥행': 48,\n",
              " '코드': 18,\n",
              " ':': 76,\n",
              " '갈등': 19,\n",
              " '-': 331,\n",
              " '계': 39,\n",
              " '화해': 4,\n",
              " '남발': 5,\n",
              " '뻔하': 139,\n",
              " '뭐': 726,\n",
              " '아햏햏': 3,\n",
              " '시작': 100,\n",
              " '3분': 5,\n",
              " '리플릿': 1,\n",
              " '사진': 13,\n",
              " '며': 229,\n",
              " '불안': 8,\n",
              " '더니만': 8,\n",
              " '단연': 12,\n",
              " '럼먹고': 1,\n",
              " '에게': 285,\n",
              " '뭘': 73,\n",
              " '엉망진창': 11,\n",
              " '개진': 1,\n",
              " '창': 29,\n",
              " '우뢰': 18,\n",
              " '매': 71,\n",
              " '진정': 82,\n",
              " '위대': 29,\n",
              " '별루': 33,\n",
              " '내일': 11,\n",
              " '`': 14,\n",
              " '조미': 2,\n",
              " '문': 11,\n",
              " '위': 53,\n",
              " '좋아하': 253,\n",
              " 'ㄴ가요': 43,\n",
              " '골깜..ㅋㅋ': 1,\n",
              " '눈': 132,\n",
              " '부라리': 1,\n",
              " '쓰러짐..ㅋㅋ': 1,\n",
              " '성룡': 56,\n",
              " '최': 85,\n",
              " '악인': 2,\n",
              " 'ㅋㅋ': 268,\n",
              " '골때리넼ㅋㅋㅋ': 1,\n",
              " '걸스데이': 1,\n",
              " '이혜리': 1,\n",
              " '서기': 5,\n",
              " '이쁘': 131,\n",
              " '재밌어요ㅋㅋㅋㅋㅋ백인공주귀여움ㅋㅋㅋㅋㅋㅋ': 1,\n",
              " '인상': 92,\n",
              " '어내스트와': 1,\n",
              " '레': 24,\n",
              " '스티': 8,\n",
              " '강': 87,\n",
              " '추': 118,\n",
              " '에요': 61,\n",
              " '클라라': 5,\n",
              " '볼라': 5,\n",
              " '화신': 2,\n",
              " '본': 42,\n",
              " '설정': 93,\n",
              " '새롭': 62,\n",
              " '메인': 5,\n",
              " '차차': 1,\n",
              " '신카이 마코토': 1,\n",
              " '작화': 6,\n",
              " '와': 641,\n",
              " '미': 103,\n",
              " '유': 44,\n",
              " '카나': 1,\n",
              " '대박': 105,\n",
              " '진심': 108,\n",
              " '1이훨나': 1,\n",
              " 'ㄴ듯': 167,\n",
              " '영화인': 44,\n",
              " '고은님': 1,\n",
              " '쓰': 337,\n",
              " '..ㅡㅡ': 1,\n",
              " '노골적': 1,\n",
              " '술': 13,\n",
              " '광고': 21,\n",
              " '어떻': 160,\n",
              " '킬링타임': 69,\n",
              " '크리스마스': 11,\n",
              " '떠오르': 16,\n",
              " 'ㅎㅎㅎ.': 1,\n",
              " '빠지': 147,\n",
              " '쪼': 23,\n",
              " '산만': 26,\n",
              " '던데': 83,\n",
              " '태어나': 27,\n",
              " '처음': 306,\n",
              " '불륜': 42,\n",
              " '로맨스': 61,\n",
              " '왕': 23,\n",
              " '아주': 126,\n",
              " '짬뽕': 10,\n",
              " '믹스': 2,\n",
              " '음향': 7,\n",
              " '덜': 35,\n",
              " '기준': 23,\n",
              " '패널': 1,\n",
              " '가구': 2,\n",
              " '머': 69,\n",
              " '거지': 74,\n",
              " '명작': 256,\n",
              " '망치': 54,\n",
              " '서운': 2,\n",
              " '이상해': 6,\n",
              " '몬스터 주식회사': 1,\n",
              " '3D': 22,\n",
              " '......': 226,\n",
              " '소재': 206,\n",
              " '흥미': 81,\n",
              " '끌': 81,\n",
              " '지만': 1049,\n",
              " '투박': 6,\n",
              " '는군': 16,\n",
              " '보지': 67,\n",
              " '마라': 47,\n",
              " '중국인': 9,\n",
              " '특유': 34,\n",
              " '과장': 14,\n",
              " '허풍': 4,\n",
              " '안간힘': 1,\n",
              " '가상': 4,\n",
              " '고증': 9,\n",
              " '현실감': 15,\n",
              " '떨어지': 112,\n",
              " '거북': 8,\n",
              " '도대체': 74,\n",
              " '까지': 603,\n",
              " '스스로': 13,\n",
              " '대포': 2,\n",
              " '장하': 2,\n",
              " '불법체류': 3,\n",
              " '때려잡': 3,\n",
              " '텐': 49,\n",
              " '우상': 1,\n",
              " '화': 88,\n",
              " 'ㄴ다고': 75,\n",
              " '미국': 93,\n",
              " '따뜻': 91,\n",
              " '뭥미??': 1,\n",
              " '2년': 4,\n",
              " '삶': 109,\n",
              " '생애': 22,\n",
              " '전부': 52,\n",
              " '드러나': 8,\n",
              " '가자': 6,\n",
              " '지나': 94,\n",
              " '후': 144,\n",
              " '남기': 68,\n",
              " '순수': 76,\n",
              " 'ㅠㅠ': 141,\n",
              " '쇼': 40,\n",
              " '펜': 11,\n",
              " '또한': 43,\n",
              " '甲': 1,\n",
              " '올레': 7,\n",
              " '공짜': 10,\n",
              " '허': 182,\n",
              " '문제': 83,\n",
              " '연기자': 30,\n",
              " '전혀': 145,\n",
              " '배역': 14,\n",
              " '어울리': 54,\n",
              " '그리고': 235,\n",
              " '상대': 10,\n",
              " '따로': 14,\n",
              " '노는거같음~~': 1,\n",
              " '라미란': 2,\n",
              " '하고': 53,\n",
              " '아들': 53,\n",
              " '젤': 25,\n",
              " '욕심': 15,\n",
              " '어느': 54,\n",
              " '쪽': 31,\n",
              " '다면': 120,\n",
              " '빵점': 22,\n",
              " '베': 26,\n",
              " '댓': 2,\n",
              " '모자': 2,\n",
              " '라진': 1,\n",
              " '도둑들': 3,\n",
              " '뫼비우스': 2,\n",
              " '나라': 37,\n",
              " '믿': 121,\n",
              " '찌릿': 1,\n",
              " '짜릿': 7,\n",
              " '용기': 17,\n",
              " '가지': 158,\n",
              " '어야지': 33,\n",
              " '교훈': 53,\n",
              " '당시': 94,\n",
              " '상황': 52,\n",
              " '주입식': 1,\n",
              " '전하': 22,\n",
              " '케이블': 36,\n",
              " '그만': 67,\n",
              " '다르': 79,\n",
              " '리': 107,\n",
              " '투': 16,\n",
              " '차이밍량': 1,\n",
              " '섞이': 7,\n",
              " '채': 26,\n",
              " '그릇': 6,\n",
              " '담기': 28,\n",
              " '여군': 3,\n",
              " '건지': 106,\n",
              " '...ㅡㅡ잼없음': 1,\n",
              " '엠비씨': 1,\n",
              " '질리': 29,\n",
              " '다이': 5,\n",
              " '제': 90,\n",
              " '한석규': 8,\n",
              " '김혜수': 5,\n",
              " '많이': 239,\n",
              " '에볼라바이러스': 1,\n",
              " '떠들석': 1,\n",
              " '성': 231,\n",
              " '어떤': 84,\n",
              " '20': 57,\n",
              " '여': 69,\n",
              " '힘들': 116,\n",
              " '정도': 340,\n",
              " '다고': 328,\n",
              " '마지막': 321,\n",
              " '후반부': 36,\n",
              " '살짝': 21,\n",
              " '아쉽': 174,\n",
              " '도면': 21,\n",
              " '만해': 13,\n",
              " '떨': 17,\n",
              " '요': 397,\n",
              " '용가리': 5,\n",
              " '진짜짱짱맨이다ㅋ': 1,\n",
              " '이제서야': 21,\n",
              " '감히': 11,\n",
              " '하나로': 17,\n",
              " '꼽': 17,\n",
              " '살': 278,\n",
              " '아야': 289,\n",
              " 'ㄹ지': 42,\n",
              " '고민': 29,\n",
              " '모건 프리먼': 2,\n",
              " '나이': 67,\n",
              " '어도': 170,\n",
              " '여전히': 18,\n",
              " '섹시': 25,\n",
              " '재방송': 3,\n",
              " '혹시나': 12,\n",
              " '답': 102,\n",
              " '여운': 144,\n",
              " '손': 39,\n",
              " '상업': 16,\n",
              " '퀄리티': 5,\n",
              " '쩌': 52,\n",
              " '충분히': 51,\n",
              " '개인': 102,\n",
              " '잔인': 74,\n",
              " '노출': 30,\n",
              " '씨': 301,\n",
              " '화끈': 22,\n",
              " '더라면': 14,\n",
              " '국산': 10,\n",
              " '아끼': 14,\n",
              " '끝내': 46,\n",
              " '지겹': 36,\n",
              " '역시': 289,\n",
              " '드니': 10,\n",
              " '일품': 16,\n",
              " '맥스': 4,\n",
              " '샘': 9,\n",
              " '죽이': 100,\n",
              " '바': 61,\n",
              " '괜찮': 224,\n",
              " '뜨': 61,\n",
              " '니까': 103,\n",
              " '찍': 159,\n",
              " '껀': 13,\n",
              " '면상': 2,\n",
              " '구만': 48,\n",
              " '자신': 98,\n",
              " '어린이': 31,\n",
              " '어리': 180,\n",
              " '동심': 6,\n",
              " '멀리': 5,\n",
              " '낫': 60,\n",
              " '무술': 11,\n",
              " '인': 47,\n",
              " '총': 42,\n",
              " '크리스토퍼': 4,\n",
              " '왈츠': 1,\n",
              " '타란티노': 5,\n",
              " '조합': 10,\n",
              " '이란': 117,\n",
              " '유명한': 3,\n",
              " '편': 274,\n",
              " '외국': 21,\n",
              " '상상': 35,\n",
              " '초월': 9,\n",
              " '유명': 32,\n",
              " '오랜만': 101,\n",
              " '재밋는영화봤네요': 1,\n",
              " '종': 16,\n",
              " '방': 56,\n",
              " '오늘': 72,\n",
              " '방도': 3,\n",
              " '방송': 57,\n",
              " '대본': 14,\n",
              " '완성도': 15,\n",
              " '고요': 28,\n",
              " '요즘': 93,\n",
              " '막장': 111,\n",
              " '지치': 6,\n",
              " '수백': 4,\n",
              " '향': 4,\n",
              " '바른': 5,\n",
              " '그리': 73,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PbI2QyVyhUl",
        "outputId": "4ffd3c11-4ec8-4359-c7a2-3c15ec2858ac"
      },
      "source": [
        "#3-4 predict input text = '안녕하세요 여러분'\n",
        "#=> preprocessing -> [0,3]\n",
        "#=> seqlength = 100\n",
        "#output => 0.63 -> 긍정에 가까운거.\n",
        "\n",
        "sentence = '오 감사합니다 신기하네요'\n",
        "tokens=komoran.morphs(sentence)\n",
        "\n",
        "voca_list = []\n",
        "for token in tokens:\n",
        "  voca_list.append(vindos[token])\n",
        "\n",
        "voca_list = [voca_list]  # Tensor 형태로 변환\n",
        "\n",
        "print(voca_list)\n",
        "voca_list=p_seq(voca_list, maxlen=maxlen)\n",
        "model.predict(voca_list)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[529, 55, 12721, 804, 33, 12721, 983]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6263014]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_2Q3tbo7lH_"
      },
      "source": [
        "\"\"\"\n",
        "불용어, 자주 등장하지 않았던, \n",
        "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '.', '..',\n",
        "                 '...', '을', '에서', '로', '것','주륵주륵','좔좔','아','휴','아이구','아이쿠','아이구','어','나','우리','저희']\n",
        "단어에 대해서 제거\n",
        "등장 횟수가 3회 미만 ->\n",
        "성능 비교\n",
        "하기전 : 78 %\n",
        "한 후 : 79 %\n",
        "불용어 제거 코드 , 성능 비교표 \n",
        "귓속말로 제출 \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}